import cv2
import mediapipe as mp
import numpy as np
#import time
#import paho.mqtt.client as paho

import paho.mqtt.client as mqtt  #import the client1
import time

def on_connect(client, userdata, flags, rc):
    if rc==0:
        client.connected_flag=True #set flag
        print("connected OK")
    else:
        print("Bad connection Returned code=",rc)

mqtt.Client.connected_flag=False#create flag in class
#broker="192.168.1.184"
broker="test.mosquitto.org"
port=1883
client = mqtt.Client("Client1")             #create new instance 
client.on_connect=on_connect  #bind call back function
client.loop_start()
print("Connecting to broker ",broker)
client.connect(broker,port)      #connect to broker
while not client.connected_flag: #wait in loop
    print("In wait loop")
    time.sleep(1)
print("in Main Loop")

answer="Nothing"
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)

mp_drawing = mp.solutions.drawing_utils

drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)
mp_facedetector = mp.solutions.face_detection
mp_draw = mp.solutions.drawing_utils

cap = cv2.VideoCapture(0)
with mp_facedetector.FaceDetection(min_detection_confidence=0.7) as face_detection:
    while cap.isOpened():
        success, image = cap.read()

        start = time.time()

        # Flip the image horizontally for a later selfie-view display
        # Also convert the color space from BGR to RGB
        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)

        # Process the image and find faces
        resultats = face_detection.process(image)

       
               
                
        # To improve performance
        image.flags.writeable = False

        # Get the result
        results = face_mesh.process(image)

        # To improve performance
        image.flags.writeable = True

        # Convert the color space from RGB to BGR
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        img_h, img_w, img_c = image.shape
        face_3d = []
        face_2d = []
        if resultats.detections:
            for id, detection in enumerate(resultats.detections):
                mp_draw.draw_detection(image, detection)
                print(id, detection)

                bBox = detection.location_data.relative_bounding_box

                h, w, c = image.shape

                boundBox = int(bBox.xmin * w), int(bBox.ymin * h), int(bBox.width * w), int(bBox.height * h)
                pourcentage= f'{int(detection.score[0]*100)}%'
                pourcentage_int=int(detection.score[0]*100)
                cv2.putText(image,pourcentage, (boundBox[0], boundBox[1] - 20), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 2)
                
                if pourcentage_int>=70:
                    if results.multi_face_landmarks:
                        for face_landmarks in results.multi_face_landmarks:
                            for idx, lm in enumerate(face_landmarks.landmark):
                                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:
                                    if idx == 1:
                                        nose_2d = (lm.x * img_w, lm.y * img_h)
                                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)

                                    x, y = int(lm.x * img_w), int(lm.y * img_h)

                                    # Get the 2D Coordinates
                                    face_2d.append([x, y])

                                    # Get the 3D Coordinates
                                    face_3d.append([x, y, lm.z])       

                            # Convert it to the NumPy array
                            face_2d = np.array(face_2d, dtype=np.float64)

                            # Convert it to the NumPy array
                            face_3d = np.array(face_3d, dtype=np.float64)

                            # The camera matrix
                            focal_length = 1 * img_w

                            cam_matrix = np.array([ [focal_length, 0, img_h / 2],
                                                    [0, focal_length, img_w / 2],
                                                    [0, 0, 1]])

                            # The distortion parameters
                            dist_matrix = np.zeros((4, 1), dtype=np.float64)

                            # Solve PnP
                            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)

                            # Get rotational matrix
                            rmat, jac = cv2.Rodrigues(rot_vec)

                            # Get angles
                            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)

                            # Get the y rotation degree
                            x = angles[0] * 360
                            y = angles[1] * 360
                            z = angles[2] * 360


                            # See where the user's head tilting
                            if y < -10:
                                text = "Looking Left"
                            elif y > 10:
                                text = "Looking Right"
                            elif x < -10:
                                text = "Looking Down"
                            elif x > 10:
                                text = "Looking Up"
                            else:
                                text = "Forward"

                            # Display the nose direction
                            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)

                            p1 = (int(nose_2d[0]), int(nose_2d[1]))
                            p2 = (int(nose_2d[0] + y * 10) , int(nose_2d[1] - x * 10))

                            cv2.line(image, p1, p2, (255, 0, 0), 3)

                            # Add the text on the image
                            cv2.putText(image, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)
                            cv2.putText(image, "x: " + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                            cv2.putText(image, "y: " + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                            cv2.putText(image, "z: " + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)



 


                            #broker="192.168.1.184"
                            #port=1883
                            #def on_publish(client,userdata,result):             #create function for callback
                             #   print("data published \n")
                              #  pass
                            #client1= paho.Client("control1")                           #create client object
                            #client1.on_publish = on_publish                          #assign function to callback
                            #client1.connect(broker,port)                                 #establish connection
                            if answer!=text:
                                client.publish("position",text,retain=1,qos=1)
                                answer=text
                            
                                print("ok")
                                #a=0
                            #else:
                                #a=a+1
                                
                             

                    end = time.time()
            #totalTime = end - start

            #fps = 1 / totalTime
            #print("FPS: ", fps)

           # cv2.putText(image, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)

           # mp_drawing.draw_landmarks(
            #            image=image,
             #           landmark_list=face_landmarks,
              #          connections=mp_face_mesh.FACE_CONNECTIONS,
               #         landmark_drawing_spec=drawing_spec,
                #        connection_drawing_spec=drawing_spec)


        cv2.imshow('Head Pose Estimation', image)

        
        if cv2.waitKey(5) & 0xFF == 27:
            break

client.loop_stop()    #Stop loop 
client.disconnect() # disconnect
cap.release()
